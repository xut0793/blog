# String 类型

[[toc]]

结论：遵循UCS-2标准以16位双字节读写

学习字符串这节内容，需要理解以下几点内容：
1. 字符串是如何转为二进制数字在计算机显示的
1. 字符、字形、编码、解码
1. Unicode / ASCII / UTF8 是什么关系
1. 在HTTP请求`Content-Type：application/json;charset=utf-8`, 在script标签中设置`<script charset="utf-8" scr=""></script>`, HTML中`<meta charset='utf-8'>`的意义
1. 字符转义格式：在XML/HTML中：`&#8733;` `&#x221d;`，在JS中使用`\u`：`\u221d`
1. 在Javascript中，用如何处理字符串在内存中的存储的

上面开篇我们就说过了，在计算机内存中储存和操作的数据都是二进制数据，并且需要根据上下文的情况判断当前二进制数据是文本还是数值或其它什么东西。上面在数值中我们讲了数值是使用64位双精度浮点数的方式在内存中存取的。这一节我们理解字符是如何转为二进制存储的。

## 概念：字符、字形、编码、解码

先说明几个概念：

你真的知道什么是字符嘛？

实际上我们在视图看到的，准确来说并不叫字符，而叫字形，即字符的图形化表示。
- 字符：是一个有名称的符号，也可以说字符是数字实体，即可以用数值编码表示的一个符号。
- 字符集：是一组字符的集合，也可以称字符表，其中每个字符都被指定了一个独有的整数，称为**码点**或称为**码位**。
- 字形：是字符的可视化图形。

对于程序员来说操作的是字符，而对于普通人看到的是字形。

比如Unicode编码规范表示百分号字符，它的名称是PERCENT SIGN，用Unicode约定的十六进制数字来表示它对应的码点是：U+0025，而我们实际看到的`%`是百分号的字形。

但不一定每个字符都有对应的字形表示，比如有些字符是存在的，有数字实体的，但没有字形，所以在视图中是不显示的。如空格字符SPACE(U+0020)、删除字符DELETE(U+007F)、换行符NewLine(U+000A)等。
> 这里遵循Unicode的约定，将字符以名称或十六进制数字表示，数字表示时带有前缀U+。在XML/HTML或JS也有另一种约定的表示规范，比如删除字符DELETE(`U+007F`)在HTML中表示`&#x007F;`，JS中表示`\u007F`，都是指同一个字符实体。具体在后面转义字符会讲。

一个字形也可能由多个字符组合而成，比如梵文、音乐符号、emoij表情等，同一个字形也可能代表不同的字符，比如`∑`可以用于表示希腊大写字母SIGMA，也可表示求和字符（SUMMATION SIGN），所以不要把字符和字形混淆。

- 编码：将数字、字符、图片等等这些高级信息转为以二进制位表示的过程称为编码；
- 解码：将二进制位数据还原为高级信息的过程称为解码。

要进行编码和解码，就像数据加密和解密一样，必然需要遵循同一种协议作为规范，在数值编码和解码中浮点数就是其中一种规范，而在字符编码和解码也存在许多规范，这些规范你可以理解为就是字符与二进制数之间的建立的一个映射表。

## ASCII编码

因为计算机最初是在美国普世的，他们的语言体系就26个英文字母，所以如果要罗列所有字符，基本也就是26个英文字母，再加上一些普遍使用的标点和特殊符号等，把这些字符收集起来，就是字符集。然后对照着这个字符集约定每个字符用什么数字表示，这个字符集跟数字的映射表就是美国最早确定的ASCII字符集，它约定了128个字符，按最大数128需要7位二进制数1111111表示，又因为计算机的基本操作单位是字节（Byte）, 即8位二进制，所以在ASCII字符编码中最高位为0。即从00000000 - 01111111来表示1-127个字符，这就是ASCII编码。

可以看到ASCII即是一个字符集也是一个字符编码方案。也就是说ASCII即约定了所表示的字符，也直接确定了字符到计算机存储的二进制编码，所以也叫ASCII编码。
```
 --------            -------------
 | 字符 | --ASCII--> | 内存二进制 | 
 --------            -------------
```
## 非ASCII编码

英语体系用128个字符编码就够了，但随着计算机在世界的普及，在其它语言体系中128个字符肯定是不够的。所以此后一段时间各个国家都根据本国的语言体系产生了很多其它的编码方案：

比如，在法语中，字母上方有注音符号，它就无法用 ASCII 码表示。于是，一些欧洲国家就决定，利用字节中闲置的最高位编入新的符号。比如，法语中的 é 的编码为130（二进制10000010）。这样一来，这些欧洲国家使用的编码体系，可以表示最多256个符号。

在亚洲国家的文字中，使用的符号就更多了，比如中国简体中文常见的编码方式是 GB2312，使用两个字节表示一个汉字，即一个汉字占用16个二进制位，并且约定两个字节的最高位都是1，以区别与ASCII编码方案的最高位为0，这样可以区别中文与西文（剩下的14个二进制位足够表示汉字字符）。

但是，这里又出现了新的问题。不同的国家有不同的字母，因此，哪怕它们都使用256个符号的编码方式，代表的字母却不一样。比如，130在法语编码中代表了é，在希伯来语编码中却代表了字母Gimel (ג)，在俄语编码中又会代表另一个符号。所以互联网要在世界范围内共享信息，就需要一种通用的编码方案。这就是Unicode编码产生的背景。

## Unicode

在1987年，由于那时存在着众多互不兼容的编码方案，如8位的ASCII码、GB3212（简体中文）、Big5码（繁体中文）等，Joe Becker(Xerox公司员工)、Lee Collins(Apple员工)和Mark Davis(Apple员工)共同发起了Unicode规范（Universal Character Set，也简称UCS，通用字符集），旨在创建一个国际化的字符集。

- 1988年发布第一版Unicode草案
- 1991年1月31日，Unicode联盟成立。致力于开发、维护、促进软件国际化标准和数据（尤其是Unicode标准）发展的非营利组织。
- 1991年10月，Unicode 1.0版发布
- 1992年6月，Unicode 2.0版发布
......
- 2018年6月5日，Unicode 11.0


Unicode最初只用两个字节即16位（0x0000 - 0xFFFF范围）来定义所有字符，由于在每个版本中都会增加新的字符，所以在1996年7月中扩展到三个字节，但设计只使用 0x000000 - 0x10FFFF区间的范围。这意味着共有1114112个字符空间，但这也足够使用，截止Unicode 5.2版本中，只分配了大约100000个字，还余下很多范围空间。

下面了解下Unicode标准中的几个概念： 

### 分层

在0-0x10FFFF整个范围内进行了层次划分，分为17层，每层称为一个平面（Plane），每层可用16位，65536个字符。

- 0x000000 - 0x00FFFF: 平面0，即BMP平面，表示多种语言的基本常用字符，日常使用编码字符都位于这个平面
- 0x010000 - 0x01FFFF: 平面1，即SMP平面，表示多种语言的辅助字符
- 0x020000 - 0x02FFFF: 平面2，即SIP平面，表示多种语言的辅助表意字符
- 平面3-13，暂未使用
- 0x0E0000 - 0x0EFFFF: 平面14，即SSP平面，表示特殊目的的字符
- 0x0F0000 - 0X0FFFFF: 平面15
- 0x100000 - 0x10FFFF: 平面16，同平面15一起作为私人使用区段的辅助区间。

以上除平面0外，平面1-16统称为辅助平面或星际平面。

因为最常用的字符都位于平面0，即基本平面。所以就以基本平面范围讲解Unicode中其它通用概念：

### 区块 Block

在平面0中，聚集了世界上绝大多数语言常用字符，所以Unicode设计者将同类语言相似的字符聚在一起，以此将平面划分为多个区块。在平面0中大约有200个区块。

- 0x000000 - 0x00001F: 控制字符
- 0x000020 - 0x00007F: 基本拉丁字母，兼容ASCII的编码
- .......省略
- 0x003400 - 0x004DBF: 中日韩统一表意文字扩展A
- 0x004DC0 - 0x004DFF: 易经六十四卦符号
- 0x004E00 - 0x009FFF: 中日韩统一表意文字
- 0x00A000 - 0x00A48F: 彝文音节
- 0x00A490 - 0x00A4CF: 彝文字根
- .......省略

> 具体可以查的[Unicode®字符百科](https://unicode-table.com/cn/)

### 码位（码点）

即区块中每个字符位置所在的数值，比如0x005144，可以直接写成0x5144（高位补0），即U+5144，表示汉字“兄”，处于基本平面中的中日韩统一表意文字区块。

这个0x5144就叫做字符“兄”的Unicode码位，或者说代码点。

现在是将字符“兄”映射到了数字编码0x5144，这个二进制数有两个字节16位，按ASCII编码的逻辑，我们理论上是可以直接向系统申请两个字节内存空间存储它的。但实际上，计算机如何知道连续的两个字节是代表一个字符的编码呢？如果是用到辅助平面的编码字符，我们需要三个字节的内存区间，计算又如何知道这三个字节要连起来代表一个字符编码呢？

这就是Unicode编码系统与ASCII编码的不同，上面我们画了图，ASCII编码因为字符数量少，只占用一个字节空间，所以可以直接将ASCII码编码进内存中。但Unicode规范虽然实现了所有字符的数字映射，但因为超出了一个字节编码，所以不能直接将编码写入内存，在将Unicode码位写入计算机内存的二进制机器码还需要进行**再编码**。自然再编码过程又需要一套通用协议，这就是UTF32 / UTF16 / UTF8 这些编码规范起作用的地方，通常也称UTF32/16/8这种编码方案是Unicode字符集的实现。

> Unicode 编码系统，可分为编码方式和实现方式两个层次。

> 但也有另外一种理解：<br>字符集和字符编码不是一个概念，字符集定义了文字和二进制的对应关系，为字符分配了唯一的编号，而字符编码规定了如何将文字的编号存储到内存中。<br>所以按这个逻辑，Unicode只是作为一个字符集的概念，所以也称Unicode字符集。而UTF8之类才是字符编码。<br>有的字符集在制定时就考虑到了编码的问题，是和编码结合在一起的，像ASCII，即叫ASCII字符集也叫ASCII编码；有的字符集只管制定字符的编号，至于怎么编码，是其他人的事情，如Unicode系统。

```
 --------               -------                 --------------------
 | 字符 | --Unicode --> | 码位 | --UTF32/16/8-- | 内存二进制（码元） | 
 --------               -------                 --------------------
```
> 码元：即一个字符实际在内存中存储的二进制数。在ASCII编码中，码位等于码元，但在Unicode系统中视不同的编码规则不同

## UTF-32

从上面码位内容可知，Unicode码位到内存机器码的主要问题是：**如何让计算机识别连续的字节是表示一个字符编码？**。解决这个问题的方案之一就是UTF32编码。

UTF32编码简单粗暴，直接将每个字符分配固定长度的内存。因为Unicode目前设计最大的码位是0x10FFFF，即32位3个字节，那就直接固定死所有字符在内存中的存储长度为三个字节32位，不足的高位补0。

如果是字符串，就是一串连续的字符序列，它们在内存中就按次序挨着存放，

这种方案最简单，直接将字符编号放入内存中即可，不需要任何转换，并且以后在字符串中定位字符、修改字符都非常容易。但缺点也很显示，就是占用内存空间大。

在互联网世界中，英文体系的字符仍占绝大多数，所以英文字母和阿拉伯数字在 Unicode 中的编号都非常小，用一个字节足以容纳（这是在Unicode设计之初，为了兼容 ASCII，在设计时刻意保留了原来 ASCII 中字符的编码），即使象形文字体系的亚洲国家，如汉字，也只需要两个字节，所以像UTF32编码方案会造成很多内存浪费。自然也就会催生更优的方案。

## UTF-16

UFT-16，是一种比较折中的编码方案，它使用 2 个或者 4 个字节来存储。

- 对于 Unicode字符码位范围在 0 ~ FFFF 之间基础平面的字符，UTF-16 使用两个字节存储，并且直接存储 Unicode 编号，不用进行编码转换，这跟 UTF-32 非常类似。
- 对于 Unicode字符码位范围在 10000~10FFFF 之间辅助平面的字符，UTF-16 使用四个字节存储。
  - 需要先将码位减去0x10000（基本平面和辅助平面的分界值）等到差值
  - 然后的具体的（从前往后）第1个字节高位固定110110，，第3个字节高位固定为110111，剩余位按顺序填充差值。

> 因为减去了0x10000，所以第1字节高位的值基本就是固定的0xD8，称为前导代理。第3个字节基本固定为0xDC，称为后尾代理。

Unicode 范围<br>十六进制 | 具体的 Unicode 字符<br>二进制（码位） | UTF-16 编码（码元）	| 编码后的字节数
--|--|--|--
0000 0000 ~ 0000 FFFF |	xxxxxxxx xxxxxxxx |	xxxxxxxx xxxxxxxx	| 2
0001 0000---0010 FFFF |	yyyy yyyy yyxx xxxx xxxx | 110110yy yyyyyyyy 110111xx xxxxxxxx | 4

示例：
字符 | Unicode码位<br>十六进制 | Unicode 字符<br>二进制（码位） | 减去分界值后差值<br>二进制 | UTF-16 编码（码元）	| 码元十六进制 |编码后的字节数
--|--|--|--|--|--|--
国 | 56FD |	0101 0110 1111 1101	| | 0101 0110 1111 1101 | 56FD | 2
🐄 |	1F404 | 0001 1111 0100 0000 0100 | 0000 1111 0100 0000 0100 | 11011000 00111101 11011100 00000100 | D8 3D DC 04 | 4

所以对ASCII编码范围内的拉丁字符，UTF16编码也会产生内存浪费，因为正常一个字节就可以存储却用了两个字节的内存空间。所以UTF-16编码方案对根本问题解决并不彻底。所以出现UTF8编码方案。

## UTF-8

UTF-8 的编码的设计逻辑很简单：如果字符用一个字节存储就可以那就申请一个字节内存空间，如果需要两个字节，那就申请两个字节空间，依次类推。所以UTF-8是一种变长的编码方案。

但它需要解决一个问题：不像UTF32是固定长度存储，取一个字符知道是三个字节，UTF16也有规则判断取一个字符是读取两个字节还是四个字节。但UTF-8采用可变长度编码，计算机如何知道一个字符该读取一个字节还是两个字节或是更多字节呢，比如汉字字符“兄” `U+5144`，怎么知道是51表示一个字符，还是5144表示一个字符呢？

所以UTF-8编码约定如下规则：
- 对于编号较小的、用一个字节足以容纳的字符，我们就可以规定这个字符编号的最高位（Bit）必须是 0，（同ASCII编码一样）；
- 对于编号较大的、要用两个字节存储的字符，我们就可以规定这个字符编号的高字节的高位必须是 110，低字节的最高位必须是 10；
- 对于编号更大的、需要三个字节存储的字符，我们就可以规定这个字符编号的高字节的高位都必须是 1110，其后每个字节高位是10。

> 即左边第一个高字节有几个1，就可解读为需要几个字节。其它字节高位固定以10开头

具体的表现形式为：

- 0xxxxxxx：单字节编码形式，这和 ASCII 编码完全一样，因此 UTF-8 是兼容 ASCII 的；
- 110xxxxx 10xxxxxx：双字节编码形式；
- 1110xxxx 10xxxxxx 10xxxxxx：三字节编码形式；
- 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx：四字节编码形式。

字符 | 具体的 Unicode 字符（码位）<br>二进制 / 十六进制 | UTF-8 编码（码元）	| 编码后的字节数
--|--|--|--
N |	01001110 / 4E |		01001110	| 1
æ |	11100110 / E6 | 	11000011 10100110 | 2
⻬ | 00101110 11101100 / 2E EC | 	11100010 10111011 10101100 | 3

程序在定位字符时，从前往后依次扫描，如果发现当前字节的最高位是 0，那么就把这一个字节作为一个字符编号。如果发现当前字节的最高位是 1，那么就继续往后扫描，如果后续字节的最高位不在是10，则停止，把之前字节合并为一个字符编码；

对于常用的字符, Unicode 码位范围是 0 ~ FFFF，用 1~3 个字节足以存储，只有及其罕见，或者只有少数地区使用的字符才需要 4~6个字节存储。所以UTF-8是目前最普遍的编码方案。但也不是最优编码方案，具体要看设计者依怀表权衡采用合适编码方案。

比如有些汉字字符，如上面的“齐”字，Unicode码位是两个字节，使用UTF-8编码，因为标识位在存在，实际在内存中存储的码元就需要三个字节。但如果使用UTF-16编码，实际存储仍然是两个字节空间。

## 宽字符和窄字符

有的编码方式采用 1~n 个字节存储，是变长的，例如 UTF-8、GB2312、GBK 等；如果一个字符使用了这种编码方式，我们就将它称为多字节字符，或者称为窄字符。

有的编码方式是固定长度的，不管字符编号大小，始终采用 n 个字节存储，例如 UTF-32、UTF-16 等；如果一个字符使用了这种编码方式，我们就将它称为宽字符。

所以Unicode 字符集可以使用窄字符的方式存储，也可以使用宽字符的方式存储。ASCII 只有一个字节，无所谓窄字符和宽字符。

## 位顺序标识（BOM）

对于定长编码方式，比如UTF32 / UTF16，还细分出两种码元具体存储的不同方式，即大尾序（big-endian）和小尾序（little-endian）储存形式。

举例说：UTF16编码基本平面的字符使用两个字节，在内存存储中，到底是高位字节+低位字节，还是低位字节+高位字节的呢？

- 大尾序方式：高位字节+低位字节，（也称大端序）（默认方式）
- 小尾序方式：低位字节+高位字节，（也称小端序）

![be.png](./images/be.png)
![le.png](./images/le.png)

示例：

```
依上面字符“齐”，unicode码位：U+2EEC
如果使用UTF-16BE大端序方式存储，码元表示为：2EEC （00101110 11101100）
如果使用UTF-16LE小端序方式存储，码元表示为：EC2E （11101100 00101110）
```
在以UTF-16编码的文件的编码开头，都会放置一个U+FEFF字符作为位顺序标识符（Byte Order Mark BOM):其中UTF-16LE以FFFE代表，UTF-16BE以FEFF代表，以显示这个文字档案是以UTF-16编码时以何种存储方式写入。

> U+FEFF字符在UNICODE中代表的意义是ZERO WIDTH NO-BREAK SPACE，顾名思义，它是个没有宽度也没有断字的空白。刚好可以用来作为标识。

但现在使用BOM来标识存储顺序很少用，因为现在广泛采用的都是UTF-8编码，它是变长编码方案，不需要BOM。对于定长的编码方案都有各自指明的UTF-16BE / UTF-16LE / UTF-32BE / UTF-32LE。所以BOM头也用不到。

## UCS-2编码

上面介绍了Unicode的几种编码方案，那在JavaScript中具体使用哪种编码方案对字符在内存中进行编码呢？答案是上述方案都没有采用。

**JS对字符采用 UCS-2编码**

这个情况是有历史原因的。

互联网还没出现的年代，曾经有两个团队，不约而同想搞统一字符集。一个是1988年成立的Unicode团队，另一个是1989年成立的UCS团队（Universal Character Set，也简称UCS，通用字符集）。等到他们发现了对方的存在，很快就达成一致：世界上不需要两套统一字符集。

1991年，两个团队决定合并字符集。也就是说，从今以后只发布一套字符集，就是Unicode，并且修订此前发布的字符集，UCS的码点将与Unicode完全一致。也是这一年合并成立了Unicode联盟。

因为UCS的开发进度快于Unicode，1990年就公布了第一套编码方法UCS-2，固定使用2个字节表示已经有码点的字符。（那个时候只有一个平面，就是基本平面，所以2个字节就够用了。）UTF-16编码迟至1996年7月才公布，明确宣布是UCS-2的超集，即基本平面字符沿用UCS-2编码，辅助平面字符定义了4个字节的表示方法。

两者的关系简单说，就是UTF-16取代了UCS-2，或者说UCS-2整合进了UTF-16。所以，现在只有UTF-16，没有UCS-2。

### 为什么JavaScript不选择更高级的UTF-16，而用了已经被淘汰的UCS-2呢？

因为在JavaScript语言出现的时候，还没有UTF-16编码。1995年5月，Brendan Eich用了10天设计了JavaScript语言；10月，第一个解释引擎问世；次年11月，Netscape正式向ECMA提交语言标准。对比UTF-16的发布时间（1996年7月），就会明白Netscape公司那时没有其他选择，只有UCS-2一种编码方法可用！

- 1990年，UCS编码发布，固定2个字节编码字符
- 1995.5，JavaScript诞生
- 1995.10，Netscape导航者浏览器中实现JS解释器
- 1996.7，UTF-16发布，取代UCS-2
- 1996.11，Netscape向ECMA提JavaScript语言标准

由于JavaScript采用UCS-2编码，也就支持UTF-16中基本平面字符的编码，并且UCS-2标准对所有字符都是2个字节。如果是辅助平面内3个字节的字符，会当作两个双字节的字符处理。

因此JavaScript的部分字符函数都受到这一点的影响，无法返回正确结果。

### String.length 字符串长度计算问题

`string.length` 对于基础平面区的字符，码位最大是16位，JS中使用UCS-2编码方案存储字符，在基本平面同UTF-16编码完全一样。在内存空间中存储的码元表示时也是两个字节16位。所以字符长度计算跟字符的码位个数、码元个数都是一致的。即一个字符长度等于两个字节。

比如:

```js
let str = 'AB';
console.log(str === "\u0041\u0042"); // 即 str = 'AB'
console.log(str.length); // 2
```
但如果某个字符的码位是超出基础平面区，在辅助平面内的，比如字形：&#128004;，它的码位是`U+1F404`，那根据UTF-16编码规范，辅助平面区的字符在内存存储时需要4个字节空间。4个字节的编码不属于UCS-2，JavaScript不认识，只会把它看作单独的两个字符。

比如，下面这个emoij字符：

```js
let str = '🐄'
console.log(str.length) // 2
console.log(str === '\ud83d\udc04') // true
```

字符 | 码位 | 码位二进制 | 减去BMP平面临界值<br>0x10000 | UTF-16编码存储的码元<br>二进制 | UTF-16编码存储的码元<br>十六进制 | 转义字符表示
-- | -- | -- | -- | -- | -- | --
🐄 | `U+1F404` |  0001 1111 0100 0000 0100 | 0000 1111 0100 0000 0100 | 11011000 00111101 11011100 00000100 | d8 3d dc 04	| `\ud83d` `\udc04`


这就造成了在HTML中直接写转义字符`&#128004;`解码显示没有问题，但如果在JS中用转义字符直接表示`\u1F404`，在解码显示会转为两个字符，所以此时如果获取这个字符的长度就会为2而不是视图上看到的1。

### ES6作出的改善

当然作为JS语言的规范ECMAScript也在不断弥补这个问题，大幅增强了Unicode支持。

1. ES6可以自动识别4字节的码点。因此，最新遍历API遍历字符串不会出错。
```js
let str = '🐄'
console.log(str.length) // 2
console.log(str === '\ud83d\udc04') // true

let num = 0
for (let s of str ) {
  num++
  console.log(s) // 🐄
}
console.log(num) // 1

```
2. 转义字符改善
```js
'🐄' === '\u1F404' // false
'🐄' === '\u{1F404}' // true
```

3. 正则表达式添加u修饰符
```js
/^.$/.test('🐄') // false
/^.$/u.test('🐄') // true
```

4. 字符串ES6处理函数

ES6新增了几个专门处理4字节码点的函数。
```js
// String.fromCodePoint()：从Unicode码点返回对应字符
// String.prototype.codePointAt()：从字符返回对应的码点
String.fromCodePoint(0x1F404) // 🐄
'🐄'.codePointAt().toString(16) // 1F404
```

5. normalize方法

允许"Unicode正规化"，即将两种方法转为同样的序列。[MDN normalize](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/String/normalize)

6. 第三方库解决 

这个问题可以通过第三库解决，例如 Punycode.js。

```js
var puny = require('puycode')
puny.ucs2.decode(str).length  // 1
```

## Unicode的解码

上面讲了字体到内存空间的编码，那在前端开发中，更多的是接收数据在web端或其它视图层，将编码的字符进行解码显示，所以需要指示解码采用哪种规范。

- 如果文件是通过HTTP(S)获取的，那么其编码由`Content-Type`响应头字段指定。如`Content-Type: text/html;charset='utf-8'`
- 对JS脚本文件，也可以在`<script>`标签指定，设置`<script charset="utf-8" scr=""></script>`
- 如果都没有保证解码规范，那最终会获取HTML中`<meta charset='utf-8'>`

所以一般建议在HTTP请求响应中设置解码规范，以及在HTML文件元信息标签中设置，而且`<meta charset='utf-8'>`标签应该位于head标签第一个元素。


## 转义字符表示

为了避免解码时解码规则不匹配，比如某个字形用UTF8 或 UTF16可能是两个不同的字符，但如果直接在HTML或JS中使用Unicode的码位表示字符，则不管解码规则如何都是正确的字形，这称为转义字符。

在XML/HTML中，直接将所需字符的码位放在`&#`和`;`中间即可使用码位转义一个字符。码位可以直接用十六进制表示也可以再转成十进制表示。如果是十六进制表示，前面需要加`x`
```
数学运算符 `∝` 的码位是 221d（十六进制）,转成十进制是8733，
所以在HTML中转义字符可以表示为：`&#x221d;` 或 `&#8733;`
```
在JS中，转义字符我们使用`\u`开头，后面接四位十六进制，所以同样的运算符在JS可以表示为：`\u221d`，如果是辅助平面内的字符`U+1F404`，因为上述JS采用UCS-2编码的问题会解码成两个字符，所以要用ES6最新的写法：`\u{1F404}`。
```js
'🐄' === '\u1F404' // false
'🐄' === '\u{1F404}' // true
```

另外在Unicode的语义中表示某个字符可以用数字实体表示，即使用`U+`开头，`U+221d`

## 参考资料

[阮一峰 字符编码笔记：ASCII，Unicode 和 UTF-8](http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html)<br>
[阮一峰 Unicode与JavaScript详解](http://www.ruanyifeng.com/blog/2014/12/unicode.html)<br>
[字符编码的概念（UTF-8、UTF-16、UTF-32都是什么鬼）](https://blog.csdn.net/guxiaonuan/article/details/78678043)<br>
[大端序和小端序](https://www.cnblogs.com/flysnail/archive/2011/10/25/2223721.html)<br>
《深入理解JavaScript》P354<br>
《JavaScript程序设计》P355